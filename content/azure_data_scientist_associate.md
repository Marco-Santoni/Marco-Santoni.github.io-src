# Intro

## Azure ML Workspace

workspaces are azure resources. include:

- compute
- notebooks
- pipelines
- data
- experiments
- models

created alongside

- storage account: files by WS + data
- application insights
- key vault
- vm
- container registry

permission: RBAC

edition
- basic (no graphic designer)
- enterprise

## Tools

Azure ML Studio
- designer (no code ML model dev)
- automated ML

Azure ML SDK

Azure ML CLI Extensions

Compute Instances
- choose VM
- store notebooks independently of VMs

VS Code - Azure ML Extension

## Experiments

Azure ML tracks run of experiments

```
...
run = experiment.start_logging()
...
run.complete()
```

- logging metrics. `run.log('name', value)`. You can review them via `RunDetails(run).show()`
- experiment output file. Example: trained models. `run.upload_file(..)`.

**Script as an experiment**. In the script, you can get the context: `run = Rune.get_context()`. To run it, you define:

- RunConfiguration: python environment
- ScriptRunConfig: associates RunConfiguration with script

# Train a ML model

## Estimators

Estimator: encapsulates a run configuration and a script configuration in a single object. Save trained model as pickle in `outputs` folder

```
estimator = Estimator(
  source_directory='experiment',
  entry_script='training.py',
  compute_target='local',
  conda_packages=['scikit-learn']
)
experiment = Experiment(workspace, name='train_experiment')
run = experiment.submit(config=estimator)
```

Framework-specific estimators simplify configurations

```
from azureml.train.sklearn import SKLearn

estimator = SKLearn(
  source_directory='experiment',
  entry_script='training.py',
  compute_target='local'
)
```

## Script parameters

Use `argparse` to read the parameters in a script (eg regularization rate). To pass a parameter to an `Estimator`:

```
estimator = SKLearn(
  source_directory='experiment',
  entry_script='training.py',
  script_params={'--reg_rate': 0.1}
  compute_target='local'
)
```

## Registering models

Once the experiment `Run` has completed, you can retrieve its outputs (eg trained model).

```
run.download_file(name='outputs/models.pkl', output_file_path='model.pkl')
```

Registering a model allows to track multiple versions of a model.

```
model = Model.register(
  workspace=ws,
  model_name='classification_model',
  model_path='model.pkl', #local path
  description='a classification model',
  tags={'dept': 'sales'},
  model_framework=Model.Framework.SCIKITLEARN,
  model_framework_version='0.20.3'
)
```

or register from run:

```
run.register_model(
  ...
  model_path='outputs/model.pkl'
  ...
  )
```

# Datastores

Abstractions of cloud data sources encapsulating the information required to connect.

You can register a data store

- via ML Studio
- via SDK

```
ws = Workspace.from_config()
blob = Datastore.register_azure_blob_container(
  workspace=ws,
  datastore_name='blob_data',
  container_name='data_container',
  account_name='az_acct',
  account_key='123456'
)
```

In the SDK, you can list data stores.

## Use datastores

Most common: Azure blob and file

```
blob_ds.upload(
  src_dir='/files',
  target_path='/data/files',
  overwrite=True
)
blob_ds.download(
  target_path='downloads',
  prefix='/data'
)
```

You pass a data reference to the script to use a datastore. Data access models

- download: contents downloaded to the compute context of experiment
- upload: files generated by experiment are uploaded after run
- mount: path of datastore mounted as remote storage (only on remote compute target)

Pass reference as script parameter:

```
data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data')
estimator = SKLearn(
  source_directory='experiment_folder',
  entry_script='training_script.py',
  compute_target='local',
  script_params={'--data_folder': data_ref}
)
```

Retrieve it in script and use it like local folder:

```
parser = argparse.ArgumentParser()
parser.add_argument('--data_folder', type='str', dest='data_folder')
args = parser.parse_args()
data_files = os.listdir(args.data_folder)
```

## Datasets

Datasets are versioned packaged data objects consumed in experiments and pipelines. Types

- tabular: read as table
- file: list of file paths

You can create dataset via Azure ML Studio or via SDK. File paths can have wildcards (`/files/*.csv`).

Once a dataset is created, you can **register** it in the workspace (available later too).

Tabular:

```
from azureml.core import Dataset

blob_ds = we.get_default_datastore()
csv_paths = [
  (blob_ds, 'data/files/current_data.csv'),
  (blob_ds, 'data/files/archive/*.csv')
]
tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)
tab_ds = tab_ds.register(workspace, name='csv_table')
```

File:

```
blob_ds = ws.get_default_datastore()
file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))
file_ds = file_ds.register(workspace=ws, name='img_files')
```

**Retrieve** a dataset

```
ws = Workspace.from_config()

# Get a dataset from workspace datasets collection
ds1 = ws.datasets['csv_table']

# Get a dataset by name from the datasets class
ds2 = Dataset.get_by_name(ws, 'img_files')
```

Datasets can be **versioned**. Create a new versioning by registering with same name and `create_new_version` property:

```
file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)
```

Retrieve specific version:

```
img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)
```
