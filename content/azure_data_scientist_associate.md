# Intro

## Azure ML Workspace

workspaces are azure resources. include:

- compute
- notebooks
- pipelines
- data
- experiments
- models

created alongside

- storage account: files by WS + data
- application insights
- key vault
- vm
- container registry

permission: RBAC

edition
- basic (no graphic designer)
- enterprise

## Tools

Azure ML Studio
- designer (no code ML model dev)
- automated ML

Azure ML SDK

Azure ML CLI Extensions

Compute Instances
- choose VM
- store notebooks independently of VMs

VS Code - Azure ML Extension

## Experiments

Azure ML tracks run of experiments

```
...
run = experiment.start_logging()
...
run.complete()
```

- logging metrics. `run.log('name', value)`. You can review them via `RunDetails(run).show()`
- experiment output file. Example: trained models. `run.upload_file(..)`.

**Script as an experiment**. In the script, you can get the context: `run = Rune.get_context()`. To run it, you define:

- RunConfiguration: python environment
- ScriptRunConfig: associates RunConfiguration with script

# Train a ML model

## Estimators

Estimator: encapsulates a run configuration and a script configuration in a single object. Save trained model as pickle in `outputs` folder

```
estimator = Estimator(
  source_directory='experiment',
  entry_script='training.py',
  compute_target='local',
  conda_packages=['scikit-learn']
)
experiment = Experiment(workspace, name='train_experiment')
run = experiment.submit(config=estimator)
```

Framework-specific estimators simplify configurations

```
from azureml.train.sklearn import SKLearn

estimator = SKLearn(
  source_directory='experiment',
  entry_script='training.py',
  compute_target='local'
)
```

## Script parameters

Use `argparse` to read the parameters in a script (eg regularization rate). To pass a parameter to an `Estimator`:

```
estimator = SKLearn(
  source_directory='experiment',
  entry_script='training.py',
  script_params={'--reg_rate': 0.1}
  compute_target='local'
)
```

## Registering models

Once the experiment `Run` has completed, you can retrieve its outputs (eg trained model).

```
run.download_file(name='outputs/models.pkl', output_file_path='model.pkl')
```

Registering a model allows to track multiple versions of a model.

```
model = Model.register(
  workspace=ws,
  model_name='classification_model',
  model_path='model.pkl', #local path
  description='a classification model',
  tags={'dept': 'sales'},
  model_framework=Model.Framework.SCIKITLEARN,
  model_framework_version='0.20.3'
)
```

or register from run:

```
run.register_model(
  ...
  model_path='outputs/model.pkl'
  ...
  )
```

# Datastores

Abstractions of cloud data sources encapsulating the information required to connect.

You can register a data store

- via ML Studio
- via SDK

```
ws = Workspace.from_config()
blob = Datastore.register_azure_blob_container(
  workspace=ws,
  datastore_name='blob_data',
  container_name='data_container',
  account_name='az_acct',
  account_key='123456'
)
```

In the SDK, you can list data stores.

## Use datastores

Most common: Azure blob and file

```
blob_ds.upload(
  src_dir='/files',
  target_path='/data/files',
  overwrite=True
)
blob_ds.download(
  target_path='downloads',
  prefix='/data'
)
```

You pass a data reference to the script to use a datastore. Data access models

- download: contents downloaded to the compute context of experiment
- upload: files generated by experiment are uploaded after run
- mount: path of datastore mounted as remote storage (only on remote compute target)

Pass reference as script parameter:

```
data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data')
estimator = SKLearn(
  source_directory='experiment_folder',
  entry_script='training_script.py',
  compute_target='local',
  script_params={'--data_folder': data_ref}
)
```

Retrieve it in script and use it like local folder:

```
parser = argparse.ArgumentParser()
parser.add_argument('--data_folder', type='str', dest='data_folder')
args = parser.parse_args()
data_files = os.listdir(args.data_folder)
```

## Datasets

Datasets are versioned packaged data objects consumed in experiments and pipelines. Types

- tabular: read as table
- file: list of file paths

You can create dataset via Azure ML Studio or via SDK. File paths can have wildcards (`/files/*.csv`).

Once a dataset is created, you can **register** it in the workspace (available later too).

Tabular:

```
from azureml.core import Dataset

blob_ds = we.get_default_datastore()
csv_paths = [
  (blob_ds, 'data/files/current_data.csv'),
  (blob_ds, 'data/files/archive/*.csv')
]
tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)
tab_ds = tab_ds.register(workspace, name='csv_table')
```

File:

```
blob_ds = ws.get_default_datastore()
file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))
file_ds = file_ds.register(workspace=ws, name='img_files')
```

**Retrieve** a dataset

```
ws = Workspace.from_config()

# Get a dataset from workspace datasets collection
ds1 = ws.datasets['csv_table']

# Get a dataset by name from the datasets class
ds2 = Dataset.get_by_name(ws, 'img_files')
```

Datasets can be **versioned**. Create a new versioning by registering with same name and `create_new_version` property:

```
file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)
```

Retrieve specific version:

```
img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)
```

# Compute Contexts

The runtime context for each experiment consists of

- *environment* for the script, which includes all packages
- *compute target* on which the environment will be deployed

## Intro to Environments

Python runs in virtual environments (eg `Conda`, `pip`). Azure creates a Docker container and creates the environment. You create environments by

- `Conda` or `pip` yaml file and load it:

```
env = Environment.from_conda_specification(name='training_env', file_path='./conda.yml')
```

- from existing `Conda` environment:

```
env = Environment.from_conda_environment(name='training_env',
                            conda_environment_name='py_env')
```

- specifying packages:

```
env = Environment('training_env')
deps = CondaDependencies.create(conda_packages=['pandas', 'numpy']
                              pip_packages=['azureml-defaults'])
env.python.conda_dependencies = deps
```

Once created, you can register the environment in the workspace.

```
env.register(workspace=ws)
```

Retrieve and assign it to a `ScriptRunConfig` or an `Estimator`

```
tr_env = Environment.get(workspace=ws, name='training_env')
estimator = Estimator(
  source_directory='experiment_folder',
  entry_script='training_script.py',
  compute_target='local',
  environment_definition=tr_env
  )
```

## Compute targets

Compute targets are physical or virtual computer on which experiments are run. Types of compute

- _local compute_: your workstation or a virtual machine
- _compute clusters_: multi-node clusters of VMs that automatically scale up or down
- _inference clusters_: to deploy models, they use containers to initiate computing
- _attached compute_: attach a VM or Databricks cluster that you already use

You can create a compute target via AML studio or via SDK. A **managed** compute target is one managed by AML. Via SDK

```
ws = Workspace.from_config()
compute_name = 'aml-cluster'
compute_config = AmlCompute.provisioning_configuration(
  vm_size='STANDARD_DS12_V2',
  min_nodes=0,
  max_nodes=4,
  vm_priority='dedicated'
  )
aml_cluster = ComputeTarget.create(we, compute_name, compute_config)
aml_cluster.wait_for_completion()
```

An **unmanaged** compute target is defined and managed outside AML. You can attach it via SDK:

```
ws = Workspace.from_config()
compute_name = 'db-cluster'
db_workspace_name = 'db_workspace'
db_resource_group = 'db_resource_group'
db_access_token = 'aocsinaocnasoivn'
db_config = DatabricksCompute.attach_configuration(
  resource_group=db_resource_group,
  workspace_name=db_workspace_name,
  access_token=db_access_token
  )
db_cluster = ComputeTarget.create(we, compute_name, db_config)
db_cluster.wait_for_completion()
```

You can check if a compute target does not exist already:

```
compute_name = 'aml_cluster'
try:
  aml_cluster = ComputeTarget(workspace=ws, name=compute_name)
except ComputeTargetException:
  # create it
  ...
```

You can use a compute target in an experiment run by specifying it as a parameter

```
compute_name = 'aml_cluster'
training_env = Environment.get(workspace=ws, name='training_env')
estimator = Estimator(
  source_directory='experiment_folder',
  entry_script='training_script.py',
  environment_definition=training_env,
  compute_target=compute_name
  )
# or specify a ComputeTarget object
training_cluster = ComputeTarget(workspace=ws, name=compute_name)
estimator = Estimator(
  source_directory='experiment_folder',
  entry_script='training_script.py',
  environment_definition=training_env,
  compute_target=training_cluster
  )
```

# Orchestrating with Pipelines

A _pipeline_ is a workflow of ml tasks in which each tasks is implemented as a _step_ (either sequential or parallel). You can combine different compute targets. Common types of step:

- _PythonScriptStep_
- _EstimatorStep_: runs an estimator
- _DataTransferStep_: uses ADF
- _DatabricksStep_
- _AdlaStep_: runs a `U-SQL` job in Azure Data Lake Analytics

Define steps:

```
step1 = PythonScriptStep(
  name='prepare data',
  source_directory='scripts',
  script_name='data_prep.py',
  compute_target='aml-cluster',
  runconfig=run_config
  )

step2 = EstimatorStep(
  name='train model',
  estimator=sk_estimator,
  compute_target='aml-cluster'
  )
```

Assign steps to pipeline:

```
train_pipeline = Pipeline(
  workspace=ws,
  steps=[step1,step2]
  )
# create experiment and run pipeline
experiment = Experiment(workspace=ws, name='training-pipeline')
pipeline_run = experiment.submit(train_pipeline)
```

## Pass data between steps

The `PipelineData` object is a special kind of `DataReference` that

- reference a location in a store
- creates a da dependency between pipelines

To pass it

- define a `PipelineData` object that references a location in a data store
- specify the object as input or output for the steps that use it
- pass the `PipelineData` object as a script parameter in steps that run scripts

Example

```
raw_ds = Dataset.get_by_name(ws, 'raw_dataset')
# Define object to pass data between steps
data_store = ws.get_default_datastore()
prepped_data = PipelineData('prepped', datastore=data_store)

step1 = PythonScriptStep(
  name='prepare data',
  source_directory='scripts',
  script_name='data_prep.py',
  compute_target='aml-cluster',
  runconfig=run_config,
  # specify dataset
  inputs = [raw_ds.as_named_input('raw_data')],
  # specify PipelineData as output
  outputs = [prepped_data],
  # script reference
  arugments = ['--folder', prepped_data]
  )

step2 = EstimatorStep(
  name='train model',
  estimator=sk_estimator,
  compute_target='aml-cluster'
  # specify PipelineData
  inputs = [prepped_data],
  # pass reference to estimator script
  estimator_entry_script_arguments = ['--folder', prepped_data]
  )
```

Inside the script, you can get reference to `PipelineData` object from the argument, and use it like  a local folder.

```
parser = argpare.ArgumentParser()
parser.add_argument('--folder', type=str, dest='folder')
args = parser.parse_args()
output_folder = args.folder

# ...

# save data to PipelineData location
os.makedirs(output_folder, exist_ok=True)
output_path = os.path.join(output_folder, 'prepped_data.csv')
df.to_csv(output_path)
```

## Reuse steps

By default, the step output from a previous pipeline run is reused without rerunning the step (if script, source directory and other params have not changed). You can control this:

```
step1 = PythonScriptStep(
  #...
  allow_reuse=False
  )
```

You can force the steps to run regardless of individual configuration:

```
pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)
```

## Publish pipelines

You can publish a pipelien to create a REST endpoint through which the pipeline can be run on demand.

```
published_pipeline = pipeline.publish(
  name='training_pipeline',
  description='Model training pipeline',
  version='1.0'
  )
```

You can view it in ML Studio and get the endpoint:

```
published_pipeline.endpoint
```

You start a published endpoint by making an HTTP request to it. You pass the authorisation header (with token) and a JSON payload specifying the experiment name. The pipeline is run asynchronously, you get the run ID as response.

## Pipeline parameters

Create a `PipelineParameter` object for each parameter. Example:

```
reg_param = PipelineParameter(name='reg_rate', default_value=0.01)
# ...
step2 = EstimatorStep(
  # ...
  estimator_entry_script_arguments=[
    '--folder', prepped,
    '--reg', reg_param
  ]
)
```

After you publish a parametrised pipeline, you can pass parameter values in the JSON payload of the REST interface. Example

```
requests.post(
  enpoint,
  headers=auth_header,
  json={
    'ExperimentName': 'run_training_pipeline',
    'ParameterAssignments': {
      'reg_rate': 0.1
    }
  }
  )
```

## Schedule pipelines

Define a `ScheduleRecurrence` and use it to create a `Schedule`.

```
daily = ScheduleRecurrence(
  frequency='Day',
  interval=1
  )
pipeline_schedule = Schedule.create(
  ws,
  name='Daily Training',
  description='train model every day',
  pipeline_id=published_pipeline.id,
  experiment_name='Training_Pipeline',
  recurrence=daily
  )
```

To schedule a pipeline to run whenever **data changes**, you must create a `Schedule` that monitors a specific path on a datastore:

```
training_datastore = Datastore(workspace=ws, name='blob_data')
pipeline_schedule = Schedule.create(
  # ...
  datastore=training_datastore,
  path_on_datastore='data/training'
  )
```

# Deploy ML Models

You can deploy ass **container** to several compute targets

- Azure ML compute instance
- Azure container instance
- Azure function
- Azure Kubernetes service
- IoT module

Steps

1. register the model
2. inference configuration
3. deployment configuration
4. deploy model

## Register the model

After training, you must register the model to Azure ML workspace.

```
classification_model = Model.register(
  workspace=ws,
  model_name='classification_model',
  model_path='model.pkl',
  description='A classification model'
  )
```

Or you can use the reference to the run:

```
run.register_model(
  model_name='classification_model',
  model_path='outputs/model.pkl',
  description='A classification model'
  )
```

## Inference configuration

The model will be deployed as a service consisting of

- a script to load the model and return predictions for submitted data
- an environment in which the script will be run

Create the _entry script_ (or _scoring script_) as a Python file including 2 functions

- `init()` called when service is initialised (load model from registry)
- `run(raw_data)` called when new data is submitted to the service (generate predictions)

Example

```
def init():
  global model
  model_path = Model.get_model_path('classification_model')
  model = joblib.load(model_path)

def run(raw_data):
  data = np.array(json.loads(raw_data)['data'])
  predictions = model.predict(data)
  # return predictions as any JSON seriazable format
  return predictions.tolist()
```

You can configure the environment using Conda. You can use a `CondaDependencies` class to create a default environment (including `azureml-defaults` and other commonly-used) and add any other required packages. You then serialize the environment to a string and save it.

```
myenv = CondaDependencies()
myenv.add_conda_package('scikit-learn')

env_file = 'service_files/env.yml'
with open(env_file, 'w') as f:
  f.write(myenv.serialize_to_string())
```

After creating the script and the environment, you combine them in an `InferenceConfig`:

```
classifier_inference_config = InferenceConfig(
  runtime='python',
  source_directory='service_files',
  entry_script='score.py',
  conda_file='env.yml'
  )
```

## Deployment configuration

Now that you have the entry script and the environment, you configure the compute service. If you deploy to an AKS cluster, you create it

```
cluster_name = 'aks-cluster'
compute_config = AksCompute.provisioning_configuration(location='eastus')
production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)
production_cluster.wait_for_completion()
```

You define the deployment configuration

```
classifier_deploy_config = AksWebservice.deploy_configuration(
  cpu_cores=1,
  memory_gb=1
)
```

## Deploy the model

```
model = ws.models['classification_model']
service = Model.deploy(
  name='classification-service',
  models=[model],
  inference_config=classifier_inference_config,
  deploy_config=classifier_deploy_config,
  deployment_target=production_cluster
  )
service.wait_for_deployment()
```

## Consuming a real-time inferencing service

For **testing**, you can use the AML SDK to call a web service through the `run` method of a `WebService` object. Typically,  you send data to `run` method in a JSON like


```
{
  'data':[
    [0.1, 0.2, 3.4],
    [0.9, 8.2, 2.5],
    ...
  ]
}
```

The response is a JSON with a prediction for each case

```
response = service.run(input_data=json_data)
predictions = json.loads(response)
```

In **production**, you use a REST endpoint. You find the endpoint of a deployed service in Azure ML studio, or by retrieving the `scoring_url` property of a `Webservice` object:

```
endpoint = service.scoring_uri
```

There are 2 kinds of **authentication**:

- key: requests are authenticated by specifying the key associated with the service
- token: requests are authenticated by providing a JSON Web Token (JWT)

By default, authentication is disabled for Azure Container Instance service (set to key-based authentication for AKS).

To make an authenticate call to the REST endpoint, you include the oey or the token in the request header.

## Troubleshooting service deployment

You can

- check the service state (should be _healty_): `service.state`
- review service logs: `service.get_logs()`
- deploy to local container
